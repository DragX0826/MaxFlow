
import os
import time
import torch
import logging
import pandas as pd
from lite_experiment_suite import MaxFlowExperiment, SimulationConfig

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# ICLR 2026 Standard Baselines (DiffDock, Gnina, etc.)
# Source: Standard PDBbind v2020 Test Set (363 targets) Results
BASELINES = {
    'DiffDock': {'top1_2A': 38.2, 'top5_2A': 46.1},
    'Gnina': {'top1_2A': 24.5, 'top5_2A': 31.0},
    'EquiBind': {'top1_2A': 13.6, 'top5_2A': 20.4},
}

# The ICLR "Golden Set" (Example targets from the 363 set)
ICLR_TARGETS = ["3PBL", "1UYD", "6LU7", "7SMV"]

def run_benchmarks():
    logger.info("üöÄ Starting MaxFlow ICLR 2026 Batch Benchmark...")
    results = []
    
    # [v75.7] Load existing results for Resumption
    output_csv = "maxflow_benchmark_results.csv"
    if os.path.exists(output_csv):
        try:
            results = pd.read_csv(output_csv).to_dict('records')
            completed_ids = [r['pdb_id'] for r in results]
            logger.info(f"   üîÑ [Resumption] Found {len(completed_ids)} completed targets. Skipping...")
        except:
            completed_ids = []
    else:
        completed_ids = []

    for pdb_id in ICLR_TARGETS:
        if pdb_id in completed_ids: continue
        
        logger.info(f"\nTarget: {pdb_id} " + "="*50)
        config = SimulationConfig(
            pdb_id=pdb_id,
            target_name=pdb_id,
            batch_size=16, # Multi-Miner Ensemble
            steps=250, # [v75.3] Sweet Spot for RK4 (Precision vs Speed)
            redocking=True, 
            blind_docking=False # Enable Ground Truth comparison for metrics
        )
        
        experiment = MaxFlowExperiment(config)
        try:
            entry = experiment.run()
            results.append(entry)
        except Exception as e:
            logger.error(f"‚ùå Target {pdb_id} failed: {e}")
            
    # Calculate Final Metric: Success Rate < 2.0A
    df = pd.DataFrame(results)
    if not df.empty:
        df['success_2A'] = df['rmsd_final'] < 2.0
        success_rate = df['success_2A'].mean() * 100
        avg_rmsd = df['rmsd_final'].mean()
        avg_energy = df['energy_final'].mean()
        
        logger.info("\n" + "#"*60)
        logger.info(f"üèÜ MAXFLOW BATCH BENCHMARK RESULTS")
        logger.info(f"   Success Rate (<2.0A): {success_rate:.1f}%")
        logger.info(f"   Avg RMSD: {avg_rmsd:.2f}A")
        logger.info(f"   Avg Energy: {avg_energy:.2f} kcal/mol")
        logger.info("#"*60)
        
        # Comparative Analysis
        logger.info("\nüìä SOTA COMPARISON (ICLR 2026 Protocol)")
        for model, metrics in BASELINES.items():
            gap = success_rate - metrics['top1_2A']
            logger.info(f"   {model:10} | Top-1 <2A: {metrics['top1_2A']}% | Gap: {gap:+.1f}%")
            
        # Save results
        df.to_csv("maxflow_benchmark_results.csv", index=False)
        logger.info("\nüíæ Results saved to maxflow_benchmark_results.csv")
    else:
        logger.warning("‚ö†Ô∏è No results gathered.")

if __name__ == "__main__":
    run_benchmarks()
