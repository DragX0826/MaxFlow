"""
Reflow Distillation Trainer (Phase 25)
Fine-tunes a student model to perform single-step generation.

Method:
1. Load dataset of pairs (x_0, x_1) generated by Teacher.
2. Objective: Minimize MSE(v_pred(t, x_t), x_1 - x_0).
3. The velocity field learns to point straight from noise to data.

Reference: Rectified Flow (ICLR 2023), Instaflow (ICLR 2024).
"""

import torch
import torch.nn as nn
import torch.optim as optim
import argparse
import os
import json
from tqdm import tqdm
from torch_geometric.loader import DataLoader
from maxflow.models.max_rl import MaxFlow
from accelerate import Accelerator

class ReflowPairDataset(torch.utils.data.Dataset):
    def __init__(self, data_path):
        print(f"Loading Reflow dataset from {data_path}...")
        # Load to CPU first, Accelerator handles device placement
        self.data = torch.load(data_path, map_location='cpu')
        print(f"Loaded {len(self.data)} pairs.")
        
    def __len__(self):
        return len(self.data)
        
    def __getitem__(self, idx):
        return self.data[idx]

def train_reflow(
    dataset_path,
    checkpoint_path,
    save_dir,
    epochs=10,
    batch_size=32,
    lr=1e-5
):
    # Mixed Precision: SOTA preference for bfloat16 if T4/A100/H100
    mixed_precision = "fp16"
    if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:
        mixed_precision = "bf16"

    accelerator = Accelerator(mixed_precision=mixed_precision)
    device = accelerator.device
    os.makedirs(save_dir, exist_ok=True)
    
    accelerator.print(f"üöÄ Reflow Distillation on {device} | Precision: {mixed_precision}")
    
    # 2. Initialize Student
    try:
        # SOTA Phase 65: Updated node_in_dim=58
        student = MaxFlow(node_in_dim=58, hidden_dim=args.hidden_dim, num_layers=args.num_layers)
        # Load checkpoint on CPU first
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        state_dict = checkpoint.get('model_state_dict', checkpoint.get('state_dict', checkpoint))
        student.load_state_dict(state_dict)
        accelerator.print("‚úÖ Student initialized from Teacher weights")
    except Exception as e:
        accelerator.print(f"‚ö†Ô∏è Could not load checkpoint ({e}). Initializing from scratch.")
        student = MaxFlow(node_in_dim=58, hidden_dim=args.hidden_dim, num_layers=args.num_layers)
        
    if hasattr(torch, "compile"):
        accelerator.print("Compiling student with torch.compile...")
        student.backbone = torch.compile(student.backbone, mode="reduce-overhead")
        
    try:
        from maxflow.utils.optimization import Muon
        optimizer = Muon(student.parameters(), lr=lr, momentum=0.95)
        accelerator.print("‚úÖ Optimizer: Muon (SOTA)")
    except ImportError:
        optimizer = torch.optim.AdamW(student.parameters(), lr=lr)
        accelerator.print("‚ö†Ô∏è Optimizer: AdamW (Fallback)")
    
    # 3. Data Loader
    dataset = ReflowPairDataset(dataset_path)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # 4. Prepare with Accelerator
    student, optimizer, loader = accelerator.prepare(student, optimizer, loader)
    
    # 5. Training Loop
    student.train()
    for epoch in range(epochs):
        total_loss = 0
        pbar = tqdm(loader, desc=f"Epoch {epoch+1}/{epochs}", disable=not accelerator.is_local_main_process)
        
        for batch in pbar:
            # Accelerator handles batch device placement
            x_0 = batch.pos_L
            x_1 = batch.target_pos
            
            optimizer.zero_grad()
            
            # Sample t ~ Logit-Normal(0, 1) (SOTA Phase 15)
            # t = sigmoid(randn)
            num_graphs = batch.num_graphs
            t_raw = torch.randn(num_graphs, device=device)
            t = torch.sigmoid(t_raw)
            
            # Broadcast t to nodes
            batch_idx = getattr(batch, 'x_L_batch', getattr(batch, 'batch', None))
            if batch_idx is not None:
                t_nodes = t[batch_idx].unsqueeze(-1)
            else:
                t_nodes = t.view(-1, 1).expand(x_0.size(0), -1)
            
            # Interpolate x_t
            x_t = t_nodes * x_1 + (1 - t_nodes) * x_0
            batch.pos_L = x_t
            
            # Forward Pass (Accelerator handles inner loss if needed, but here simple)
            v_pred, _, _ = student.flow.backbone(t, batch)
            v_target = x_1 - x_0
            loss = torch.nn.functional.mse_loss(v_pred, v_target)
            
            # Accelerator Backward
            accelerator.backward(loss)
            optimizer.step()
            
            total_loss += loss.item()
            if accelerator.is_local_main_process:
                pbar.set_postfix(loss=loss.item())
            
        avg_loss = total_loss / len(loader)
        accelerator.print(f"Epoch {epoch+1}: Avg Loss = {avg_loss:.6f}")
        
        # Save Checkpoint
        if accelerator.is_main_process:
            unwrapped_student = accelerator.unwrap_model(student)
            torch.save({
                'model_state_dict': unwrapped_student.state_dict(),
                'epoch': epoch,
                'loss': avg_loss
            }, os.path.join(save_dir, f"reflow_student_epoch_{epoch+1}.pt"))

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--checkpoint", type=str, required=True)
    parser.add_argument("--save_dir", type=str, default="checkpoints_reflow")
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--lr", type=float, default=1e-5)
    parser.add_argument("--hidden_dim", type=int, default=128)
    parser.add_argument("--num_layers", type=int, default=4)
    args = parser.parse_args()
    
    train_reflow(
        args.dataset, args.checkpoint, args.save_dir, 
        epochs=args.epochs, batch_size=args.batch_size, lr=args.lr
    )



